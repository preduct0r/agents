{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import autogen\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = Path(\"/home/kotov-d@ad.speechpro.com/projects/AGENTS/data/result.csv\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df[df[\"file_path\"] == '/mnt/cs/nlu/home/kotov/LLM/auto_validation/data_from_stc_cloud/mosmetro_hand_tests_semantic.xlsx']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Кейс', 'Вопрос пользователя', 'Референсный ответ', 'Чанк 1', 'Чанк 2',\n",
       "       'Чанк 3', 'Чанк 4', 'Ответ модели', 'Качество поиска',\n",
       "       'Понимание задания', 'Соответствие док-там', 'Непротиворечивость',\n",
       "       'Полнота', 'Итог', 'Разметчик', 'Комментарий', 'file_path',\n",
       "       'Референсный документ', 'Референсный документ 2',\n",
       "       'helpsteer-helpfulness', 'helpsteer-correctness', 'helpsteer-coherence',\n",
       "       'helpsteer-complexity', 'helpsteer-verbosity',\n",
       "       'giga_neprotivirechivost', 'giga_polnota', 'gpt4o_neprotivirechivost',\n",
       "       'gpt4o_polnota', 'llama70b_neprotivorechivost', 'llama70b_polnota'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"/mnt/hs/dorado6/models/meta/llama-3-70b-instruct\", # Sam2,e as in vLLM command\n",
    "            \"api_key\": \"NotRequired\", # Not needed\n",
    "            \"base_url\": \"http://0.0.0.0:8000/v1\"  # Your vLLM URL, with '/v1' added\n",
    "        }\n",
    "    ],\n",
    "    \"cache_seed\": None # Turns off caching, useful for testing different models\n",
    "}\n",
    "\n",
    "llm_config = {\"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": OPENAI_API_KEY}], \"cache_seed\": 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 2,\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },  \n",
    "    human_input_mode=\"TERMINATE\",\n",
    ")\n",
    "spk1 = autogen.AssistantAgent(\n",
    "    name=\"Speaker1\",\n",
    "    system_message=\"Эксперт в российской железнодорожной индустрии\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "spk2 = autogen.AssistantAgent(\n",
    "    name=\"Speaker2\",\n",
    "    system_message=\"Эксперт в российской железнодорожной индустрии. Отвечай только по-русски\",\n",
    "    llm_config=local_llm_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, spk1, spk2], messages=[], max_round=3)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_result = user_proxy.initiate_chat(\n",
    "#     manager, message=f\"\"\"Непротиворечит ли ответ модели правильному ответу? Порассуждайте с коллегами на эту тему, аргументируя свое мнение, и выведите результат вашей дискуссии. 0 если ответ модели полностью противоречит референсному ответу, 1- если ответ модели частично противоречит референсному ответу и здравому смыслу, 2- результат не противоречит референсному и здравому смыслуответу  \\n**Вопрос**\\n{df.loc[148]['Вопрос пользователя']}\\n\\n**Ответ модели:**\\n{df.loc[148]['Ответ модели']}\\n\\n**Референсный ответ:**\\n{df.loc[148]['Референсный ответ']}\\n\"\"\"\n",
    "# )\n",
    "# # type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Кейс', 'Вопрос пользователя', 'Референсный ответ', 'Чанк 1', 'Чанк 2',\n",
       "       'Чанк 3', 'Чанк 4', 'Ответ модели', 'Качество поиска',\n",
       "       'Понимание задания', 'Соответствие док-там', 'Непротиворечивость',\n",
       "       'Полнота', 'Итог', 'Разметчик', 'Комментарий', 'file_path',\n",
       "       'Референсный документ', 'Референсный документ 2',\n",
       "       'helpsteer-helpfulness', 'helpsteer-correctness', 'helpsteer-coherence',\n",
       "       'helpsteer-complexity', 'helpsteer-verbosity',\n",
       "       'giga_neprotivirechivost', 'giga_polnota', 'gpt4o_neprotivirechivost',\n",
       "       'gpt4o_polnota', 'llama70b_neprotivorechivost', 'llama70b_polnota'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries, histories = [], []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    try:\n",
    "        chat_result = user_proxy.initiate_chat(\n",
    "            manager, message=f\"\"\"Непротиворечит ли ответ модели правильному ответу? Порассуждайте с коллегами на эту тему, аргументируя свое мнение, и выведите результат вашей дискуссии. 0 если ответ модели полностью противоречит референсному ответу, 1- если ответ модели частично противоречит референсному ответу или здравому смыслу, 2- результат не противоречит референсному ответу или здравому смыслу  \\n**Вопрос**\\n{df.loc[idx]['Вопрос пользователя']}\\n\\n**Ответ модели:**\\n{df.loc[idx]['Ответ модели']}\\n\\n**Референсный ответ:**\\n{df.loc[idx]['Референсный ответ']}\\n\"\"\"\n",
    "        )\n",
    "        summaries.append(chat_result.summary)\n",
    "        histories.append(json.dumps(chat_result.chat_history, indent=2, ensure_ascii=False))\n",
    "    except:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)\n",
    "# df.iloc[28:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_results = df.drop(index=138).copy()\n",
    "agent_results[\"agent_summary\"] = summaries\n",
    "agent_results[\"agent_history\"] = histories\n",
    "agent_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(tail):\n",
    "    if '0' in tail:\n",
    "        return 0\n",
    "    elif '2' in tail:\n",
    "        return 2\n",
    "    elif '1' in tail:\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def in_tail(tail):\n",
    "    to_return = set()\n",
    "    if '0' in tail:\n",
    "        to_return.add('0')\n",
    "    if '1' in tail:\n",
    "        to_return.add('1')\n",
    "    if '2' in tail:\n",
    "        to_return.add('2')\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 12, None: 6, 0: 6, 2: 6})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_res = []\n",
    "\n",
    "for i in agent_results[\"agent_summary\"]:\n",
    "    tail = i[-200:]\n",
    "    \n",
    "    if len(in_tail(tail).intersection(set(['0','1','2']))) > 1:\n",
    "        tail = tail[-100:]\n",
    "        if len(in_tail(tail).intersection(set(['0','1','2']))) > 1:\n",
    "            score = get_score(tail[-50:])\n",
    "        else:\n",
    "            score = get_score(tail)\n",
    "    else:\n",
    "        score = get_score(tail)\n",
    "\n",
    "    agent_res.append(score)\n",
    "        \n",
    "Counter(agent_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent_results[\"agent_score\"] = agent_res\n",
    "# len(agent_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Кейс', 'Вопрос пользователя', 'Референсный ответ', 'Чанк 1', 'Чанк 2',\n",
       "       'Чанк 3', 'Чанк 4', 'Ответ модели', 'Качество поиска',\n",
       "       'Понимание задания', 'Соответствие док-там', 'Непротиворечивость',\n",
       "       'Полнота', 'Итог', 'Разметчик', 'Комментарий', 'file_path',\n",
       "       'Референсный документ', 'Референсный документ 2',\n",
       "       'helpsteer-helpfulness', 'helpsteer-correctness', 'helpsteer-coherence',\n",
       "       'helpsteer-complexity', 'helpsteer-verbosity',\n",
       "       'giga_neprotivirechivost', 'giga_polnota', 'gpt4o_neprotivirechivost',\n",
       "       'gpt4o_polnota', 'llama70b_neprotivorechivost', 'llama70b_polnota',\n",
       "       'agent_summary', 'agent_history'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"/home/kotov-d@ad.speechpro.com/projects/AGENTS/data/agent_result_gpt_llama.csv\")\n",
    "EXCEL_PATH = Path(\"/home/kotov-d@ad.speechpro.com/projects/AGENTS/data/agent_result_gpt_llama.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_results.to_csv(CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_results[agent_results[\"agent_score\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_string = agent_results[agent_results[\"agent_score\"].isna()].iloc[0,:][\"agent_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Непротиворечит ли ответ модели правильному ответу? Порассуждайте с коллегой на эту тему и выведите результат вашей дискуссии. Ответ должен быть строго в формате \"Ответ: [VALUE]\", где [VALUE], это 0 если ответ модели полностью противоречит референсному ответу, 1- если ответ модели частично противоречит референсному ответу и здравому смыслу, 2- результат не противоречит референсному и здравому смыслуответу  \\n**Вопрос**\\nМне нужно съездить на метро до ВДНХ и обратно. Выгодно ли брать «единый» на сутки?\\n\\n**Ответ модели:**\\nЕсли вы планируете поездку до ВДНХ и обратно, то выгоднее взять единый на сутки. Два билета стоят 124 рубля, а единый на сутки стоит 285 рублей. Однако, если вы собираетесь совершить больше двух поездок или планируете использовать другие виды транспорта, то может быть выгоднее приобрести другие типы билетов или использовать карту \"Тройка\". Подробнее о тарифах и правилах проезда можно узнать на официальном сайте Московского Метро https://mosmetro.ru/passengers/information/ticket-rules/.\\n\\n**Референсный ответ:**\\nПроезд можно оплатить картой «Тройка», бумажным билетом «Единый», социальной картой, банковской картой, с помощью Системы быстрых платежей (только на станциях БКЛ), смартфоном или часами, биометрией.\\nОдна поездка по «Тройке» стоит **50 рублей**.\\nОдна поездка по «бумажному» билету «Единый» стоит **62 рубля**.\\nОдна поездка, оплаченная картой или смартфоном/часами, – **56 рублей**.\\n**Дополнительные сборы не взимаются.**\\n\\nВы можете купить «Единый» с лимитом или без лимита поездок (на сутки, трое суток, 30, 90 и 365 дней).\\nБезлимитный «Единый» на все виды транспорта без зоны «Пригород» МЦД стоит 285 руб. (1 сутки), 540 руб. (3 суток), 2540 руб. (30 дней), 6150 руб. (90 дней) и 19 500 руб. (365 дней).\\n«Единый» с зоной «Пригород» МЦД стоит 3010 руб. (30 дней), 7800 руб. (90 дней) и 24 450 руб. (365 дней). Билеты на сутки и трое суток с зоной «Пригород» со 2 января 2022 года отменили.\\n«Единые» можно записать на «Тройку» в любой кассе метро, МЦК, в пригородных ж/д кассах, в приложении «Метро Москвы» или в приложениях «Метро Москвы» и «Московский транспорт».\\n',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Ответ: 2\\n\\n**Рассуждение:**\\nОтвет модели и референсный ответ предоставляют информацию, которая не противоречит друг другу. Модель указывает на стоимость билетов и сравнивает их, что согласуется с референсным ответом, предоставляющим детальную информацию о различных способах оплаты проезда и их стоимости. Советы модели об экономической целесообразности тоже не противоречат здравому смыслу и исходят из представленных фактов.',\n",
       "  'name': 'Speaker1',\n",
       "  'role': 'user'},\n",
       " {'content': '', 'role': 'assistant'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_results.loc[7, \"agent_score\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_results = agent_results.fillna(\"None\")\n",
    "# agent_results.to_csv(CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_results = pd.read_csv(CSV_PATH)\n",
    "\n",
    "sub_df = agent_results.loc[:, [\"agent_history\",'Непротиворечивость', 'gpt4o_neprotivirechivost']]\n",
    "sub_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs, llamas, gpts = [], [], []\n",
    "for json_string in sub_df[\"agent_history\"]:\n",
    "    cur_data = json.loads(json_string)\n",
    "    qs.append(cur_data[0]['content'])\n",
    "    gpt_ans = cur_data[1]['content']\n",
    "    gpts.append(gpt_ans)\n",
    "    llama_ans = cur_data[2]['content']\n",
    "    llamas.append(llama_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=[\"question\", \"gpt_1_turn\", \"llama_2_turn\"])\n",
    "new_df[\"question\"] = qs\n",
    "new_df[\"gpt_1_turn\"] = gpts\n",
    "new_df[\"llama_2_turn\"] = llamas\n",
    "\n",
    "new_df['agent_score'] = None\n",
    "new_df['Непротиворечивость'] = agent_results.loc[:, ['Непротиворечивость']]\n",
    "new_df['gpt4o_neprotivirechivost'] = agent_results.loc[:, ['gpt4o_neprotivirechivost']]\n",
    "\n",
    "new_df = new_df.fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xlsxwriter\n",
    "workbook = xlsxwriter.Workbook(EXCEL_PATH)\n",
    "worksheet = workbook.add_worksheet(f\"вопрос {1}\")\n",
    "\n",
    "worksheet.set_column('A:A', 80)  \n",
    "worksheet.set_column('B:B', 80)  \n",
    "worksheet.set_column('C:C', 80)  \n",
    "worksheet.set_column('D:D', 10)  \n",
    "worksheet.set_column('E:E', 10)  \n",
    "worksheet.set_column('F:F', 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_format = workbook.add_format({\n",
    "            'border': 1,  # Add a border\n",
    "            'align': 'left',  # Align text to the left\n",
    "            'valign': 'top',  # Center text vertically\n",
    "            'text_wrap': True  # Enable text wrapping\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:00, 11082.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for row_num, row in tqdm(new_df.iterrows()):\n",
    "    worksheet.write_row(row_num, 0, row, border_format)\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
